[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tidy Finance with R: Some notes",
    "section": "",
    "text": "Preface\nThis “book” is a collection of notes made as I work through Tidy Finance with R."
  },
  {
    "objectID": "index.html#data",
    "href": "index.html#data",
    "title": "Tidy Finance with R: Some notes",
    "section": "Data",
    "text": "Data\n\n\n\nTable\nChapter\nCode\n\n\n\n\nbeta\n6\nmake_betas.R\n\n\ncompustat\n3\nmake_compustat.R\n\n\ncpi_monthly\n2\nfin_data.R\n\n\ncrsp_daily\n3\nmake_crsp_daily.R\n\n\ncrsp_monthly\n3\nmake_crsp_monthly.R\n\n\nfactors_ff_daily\n2\nfin_data.R\n\n\nfactors_ff_monthly\n2\nfin_data.R\n\n\nfactors_q_monthly\n2\nfin_data.R\n\n\nindustries_ff_monthly\n2\nfin_data.R\n\n\nmacro_predictors\n2\nfin_data.R\n\n\nmergent\n\n\n\n\ntrace_enhanced"
  },
  {
    "objectID": "chapter_1.html#scaling-up-the-analysis",
    "href": "chapter_1.html#scaling-up-the-analysis",
    "title": "1  Introduction to Tidy Finance",
    "section": "1.1 Scaling Up the Analysis",
    "text": "1.1 Scaling Up the Analysis\n\nticker <- tq_index(\"DOW\")\n\nGetting holdings for DOW\n\nticker\n\n# A tibble: 31 × 8\n   symbol company      identifier sedol weight sector shares_held local_currency\n   <chr>  <chr>        <chr>      <chr>  <dbl> <chr>        <dbl> <chr>         \n 1 UNH    UNITEDHEALT… 91324P102  2917… 0.0966 -          5504172 USD           \n 2 GS     GOLDMAN SAC… 38141G104  2407… 0.0634 -          5504172 USD           \n 3 MSFT   MICROSOFT C… 594918104  2588… 0.0613 -          5504172 USD           \n 4 MCD    MCDONALD S … 580135101  2550… 0.0583 -          5504172 USD           \n 5 HD     HOME DEPOT … 437076102  2434… 0.0569 -          5504172 USD           \n 6 AMGN   AMGEN INC    031162100  2023… 0.0459 -          5504172 USD           \n 7 V      VISA INC CL… 92826C839  B2PZ… 0.0457 -          5504172 USD           \n 8 CAT    CATERPILLAR… 149123101  2180… 0.0415 -          5504172 USD           \n 9 CRM    SALESFORCE … 79466L302  2310… 0.0402 -          5504172 USD           \n10 BA     BOEING CO/T… 097023105  2108… 0.0399 -          5504172 USD           \n# ℹ 21 more rows\n\n\n\nindex_prices <-\n  tq_get(ticker,\n         get = \"stock.prices\",\n         from = \"2000-01-01\",\n         to = \"2022-12-31\")\n\nWarning: There was 1 warning in `dplyr::mutate()`.\nℹ In argument: `data.. = purrr::map(...)`.\nCaused by warning:\n! x = '-', get = 'stock.prices': Error in getSymbols.yahoo(Symbols = \"-\", env = <environment>, verbose = FALSE, : Unable to import \"-\".\nHTTP error 404.\n Removing -.\n\n\n\nindex_prices |>\n  filter(date <= \"2022-09-30\") |>\n  ggplot(aes(\n    x = date,\n    y = adjusted,\n    color = symbol\n  )) +\n  geom_line() +\n  labs(\n    x = NULL,\n    y = NULL,\n    color = NULL,\n    title = \"Stock prices of DOW index constituents\"\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\n\nall_returns <- \n  index_prices |>\n  group_by(symbol) |>\n  mutate(ret = adjusted / lag(adjusted) - 1) |>\n  select(symbol, date, ret) |>\n  drop_na(ret)\n\nall_returns |>\n  mutate(ret = ret * 100) |>\n  group_by(symbol) |>\n  summarize(across(\n    ret,\n    list(\n      daily_mean = mean,\n      daily_sd = sd,\n      daily_min = min,\n      daily_max = max\n    ),\n    .names = \"{.fn}\"\n  )) |>\n  print(n = Inf)\n\n# A tibble: 30 × 5\n   symbol daily_mean daily_sd daily_min daily_max\n   <chr>       <dbl>    <dbl>     <dbl>     <dbl>\n 1 AAPL       0.120      2.51     -51.9      13.9\n 2 AMGN       0.0489     1.97     -13.4      15.1\n 3 AXP        0.0518     2.29     -17.6      21.9\n 4 BA         0.0595     2.24     -23.8      24.3\n 5 CAT        0.0709     2.04     -14.5      14.7\n 6 CRM        0.110      2.70     -27.1      26.0\n 7 CSCO       0.0317     2.37     -16.2      24.4\n 8 CVX        0.0553     1.76     -22.1      22.7\n 9 DIS        0.0418     1.95     -18.4      16.0\n10 DOW        0.0562     2.60     -21.7      20.9\n11 GS         0.0550     2.31     -19.0      26.5\n12 HD         0.0543     1.94     -28.7      14.1\n13 HON        0.0515     1.94     -17.4      28.2\n14 IBM        0.0273     1.65     -15.5      12.0\n15 INTC       0.0285     2.36     -22.0      20.1\n16 JNJ        0.0408     1.22     -15.8      12.2\n17 JPM        0.0582     2.42     -20.7      25.1\n18 KO         0.0337     1.32     -10.1      13.9\n19 MCD        0.0533     1.47     -15.9      18.1\n20 MMM        0.0378     1.50     -12.9      12.6\n21 MRK        0.0383     1.68     -26.8      13.0\n22 MSFT       0.0513     1.94     -15.6      19.6\n23 NKE        0.0743     1.94     -19.8      15.5\n24 PG         0.0377     1.34     -30.2      12.0\n25 TRV        0.0569     1.83     -20.8      25.6\n26 UNH        0.0984     1.98     -18.6      34.8\n27 V          0.0929     1.90     -13.6      15.0\n28 VZ         0.0239     1.51     -11.8      14.6\n29 WBA        0.0284     1.82     -15.0      16.6\n30 WMT        0.0314     1.50     -11.4      11.7"
  },
  {
    "objectID": "chapter_1.html#other-forms-of-data-aggregation",
    "href": "chapter_1.html#other-forms-of-data-aggregation",
    "title": "1  Introduction to Tidy Finance",
    "section": "1.2 Other Forms of Data Aggregation",
    "text": "1.2 Other Forms of Data Aggregation\n\nvolume <- \n  index_prices |>\n  group_by(date) |>\n  summarize(volume = sum(volume * close / 1e9))\n\nvolume |>\n  ggplot(aes(x = date, y = volume)) +\n  geom_line() +\n  labs(\n    x = NULL, y = NULL,\n    title = \"Aggregate daily trading volume of DOW index constitutens\"\n  )\n\n\n\n\n\nvolume |>\n  ggplot(aes(x = lag(volume), y = volume)) +\n  geom_point() +\n  geom_abline(aes(intercept = 0, slope = 1),\n              linetype = \"dashed\"\n  ) +\n  labs(\n    x = \"Previous day aggregate trading volume\",\n    y = \"Aggregate trading volume\",\n    title = \"Persistence in daily trading volume of DOW index constituents\"\n  )\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n\n\n\n\nindex_prices <- \n  index_prices |>\n  group_by(symbol) |>\n  mutate(n = n()) |>\n  ungroup() |>\n  filter(n == max(n)) |>\n  select(-n)"
  },
  {
    "objectID": "chapter_1.html#portfolio-choice-problems",
    "href": "chapter_1.html#portfolio-choice-problems",
    "title": "1  Introduction to Tidy Finance",
    "section": "1.3 Portfolio Choice Problems",
    "text": "1.3 Portfolio Choice Problems\n\nreturns <- \n  index_prices |>\n  mutate(month = floor_date(date, \"month\")) |>\n  group_by(symbol, month) |>\n  summarize(price = last(adjusted), .groups = \"drop_last\") |>\n  mutate(ret = price / lag(price) - 1) |>\n  drop_na(ret) |>\n  select(-price)\n\nreturns_matrix <- \n  returns |>\n  pivot_wider(\n    names_from = symbol,\n    values_from = ret\n  ) |>\n  select(-month)\n\nSigma <- cov(returns_matrix)\nmu <- colMeans(returns_matrix)\n\nN <- ncol(returns_matrix)\niota <- rep(1, N)\nmvp_weights <- solve(Sigma) %*% iota\nmvp_weights <- mvp_weights / sum(mvp_weights)\ntibble(\n  average_ret = as.numeric(t(mvp_weights) %*% mu),\n  volatility = as.numeric(sqrt(t(mvp_weights) %*% Sigma %*% mvp_weights))\n)\n\n# A tibble: 1 × 2\n  average_ret volatility\n        <dbl>      <dbl>\n1     0.00792     0.0321\n\nmu_bar <- 3 * t(mvp_weights) %*% mu\n\nC <- as.numeric(t(iota) %*% solve(Sigma) %*% iota)\nD <- as.numeric(t(iota) %*% solve(Sigma) %*% mu)\nE <- as.numeric(t(mu) %*% solve(Sigma) %*% mu)\n\nlambda_tilde <- as.numeric(2 * (mu_bar - D / C) / (E - D^2 / C))\nefp_weights <- mvp_weights +\n  lambda_tilde / 2 * (solve(Sigma) %*% mu - D * mvp_weights)"
  },
  {
    "objectID": "chapter_1.html#the-efficient-frontier",
    "href": "chapter_1.html#the-efficient-frontier",
    "title": "1  Introduction to Tidy Finance",
    "section": "1.4 The Efficient Frontier",
    "text": "1.4 The Efficient Frontier\n\nc <- seq(from = -0.4, to = 1.9, by = 0.01)\n\nres <- tibble(\n  c = c,\n  mu = NA,\n  sd = NA\n)\nfor (i in seq_along(c)) {\n  w <- (1 - c[i]) * mvp_weights + (c[i]) * efp_weights\n  res$mu[i] <- 12 * 100 * t(w) %*% mu   \n  res$sd[i] <- 100 * sqrt(12) * sqrt(t(w) %*% Sigma %*% w)\n}\n\n\nget_res <- function(c) {\n  w <- (1 - c) * mvp_weights + (c) * efp_weights\n  mu_star <- 12 * 100 * t(w) %*% mu   \n  sd_star <- 100 * sqrt(12) * sqrt(t(w) %*% Sigma %*% w)\n  data.frame(c = c, mu = mu_star, sd = sd_star)\n}\nsystem.time({\n  res <- bind_rows(lapply(c, get_res))\n})\n\n   user  system elapsed \n  0.017   0.001   0.017 \n\nres |>\n  ggplot(aes(x = sd, y = mu)) +\n  geom_point() +\n  geom_point(\n    data = res |> filter(c %in% c(0, 1)),\n    size = 4\n  ) +\n  geom_point(\n    data = tibble(\n      mu = 12 * 100 * mu,       \n      sd = 100 * sqrt(12) * sqrt(diag(Sigma))\n    ),\n    aes(y = mu, x = sd), size = 1\n  ) +\n  labs(\n    x = \"Annualized standard deviation (in percent)\",\n    y = \"Annualized expected return (in percent)\",\n    title = \"Efficient frontier for DOW index constituents\"\n  )"
  },
  {
    "objectID": "fin_data.html",
    "href": "fin_data.html",
    "title": "2  Accessing & Managing Financial Data",
    "section": "",
    "text": "The code for this chapter is found in fin_data.R."
  },
  {
    "objectID": "wrds.html",
    "href": "wrds.html",
    "title": "3  WRDS, CRSP, and Compustat",
    "section": "",
    "text": "library(tidyverse)\nlibrary(DBI)\nlibrary(scales)\n\n\ntidy_finance <- dbConnect(\n  duckdb::duckdb(),\n  \"data/tidy_finance.duckdb\",\n  read_only = TRUE)\n\n\ncrsp_monthly <- tbl(tidy_finance, \"crsp_monthly\")\ncpi_monthly <- tbl(tidy_finance, \"cpi_monthly\")\ncompustat  <- tbl(tidy_finance, \"compustat\")\n\n\ncrsp_monthly |>\n  left_join(cpi_monthly, by = \"month\") |>\n  group_by(month, exchange) |>\n  summarize(\n    mktcap = sum(mktcap/cpi, na.rm = TRUE),\n    .groups = \"drop\"\n  ) |>\n  ggplot(aes(\n    x = month, y = mktcap / 1000,\n    color = exchange, linetype = exchange\n  )) +\n  geom_line() +\n  labs(\n    x = NULL, y = NULL, color = NULL, linetype = NULL,\n    title = \"Monthly market cap by listing exchange in billions of Dec 2021 USD\"\n  ) +\n  scale_x_date(date_breaks = \"10 years\", date_labels = \"%Y\") +\n  scale_y_continuous(labels = comma)\n\n\n\n\n\ncrsp_monthly |>\n  group_by(permno, year = year(month)) |>\n  filter(date == max(date)) |>\n  ungroup() |>\n  left_join(compustat, by = c(\"gvkey\", \"year\")) |>\n  mutate(permno_be = if_else(!is.na(be), permno, NA)) |>\n  group_by(exchange, year) |>\n  summarize(\n    share = 1.0 * n_distinct(permno_be) / n_distinct(permno),\n    .groups = \"drop\") |>\n  ggplot(aes(\n    x = year, \n    y = share, \n    color = exchange,\n    linetype = exchange\n  )) +\n  geom_line() +\n  labs(\n    x = NULL, y = NULL, color = NULL, linetype = NULL,\n    title = \"Share of securities with book equity values by exchange\"\n  ) +\n  scale_y_continuous(labels = percent) +\n  coord_cartesian(ylim = c(0, 1))\n\nWarning: Missing values are always removed in SQL aggregation functions.\nUse `na.rm = TRUE` to silence this warning\nThis warning is displayed once every 8 hours."
  },
  {
    "objectID": "betas.html#code-to-create-beta",
    "href": "betas.html#code-to-create-beta",
    "title": "4  Beta Estimation",
    "section": "4.1 Code to create beta",
    "text": "4.1 Code to create beta\nInstead of the code in the book, I used the following code to create beta (here renamed beta_alt). The code from the book (see here) takes 12 minutes and maxes out at about 37GB of RAM. The code below takes 25 seconds and peaks at about 7GB.\nThe code below is perhaps uglier than it needs to be, but is designed to get as close as possible to the results of the code in the book. Note that I get some differences with beta_monthly, but the same values for beta_daily.\n\nlibrary(tidyverse)\nlibrary(DBI)\nlibrary(dbplyr)\n\ntidy_finance <- dbConnect(\n  duckdb::duckdb(),\n  \"data/tidy_finance.duckdb\",\n  read_only = FALSE)\n\ncrsp_monthly <- tbl(tidy_finance, \"crsp_monthly\") \ncrsp_daily <- tbl(tidy_finance, \"crsp_daily\") \n\nfactors_ff_monthly <- tbl(tidy_finance, \"factors_ff_monthly\")\nfactors_ff_daily <- tbl(tidy_finance, \"factors_ff_daily\")\n\nwindow <- \"OVER (PARTITION BY permno ORDER BY month\n           RANGE BETWEEN INTERVAL 59 MONTHS PRECEDING\n              AND INTERVAL 0 MONTHS FOLLOWING)\"\n\nbeta_sql <- sql(paste(\"regr_slope(ret_excess, mkt_excess)\", window))\nn_sql <- sql(paste(\"count(*)\", window))\nmax_sql <- sql(paste(\"max(month)\", window))\nmin_sql <- sql(paste(\"min(month)\", window))\n\nbeta_monthly <- \n  crsp_monthly |>\n  left_join(factors_ff_monthly, by = \"month\") |>\n  mutate(beta_monthly = beta_sql,\n         n = n_sql,\n         max_month = max_sql,\n         min_month = min_sql) |>\n  ungroup() |>\n  # Check that months are consecutive\n  mutate(n_months = date_diff('month', min_month, max_month) + 1) |>\n  filter(n >= 48, n_months == n) |>\n  select(permno, month, beta_monthly) |>\n  filter(!is.na(beta_monthly)) %>%\n  compute()\n\nwindow <- \"OVER (PARTITION BY permno ORDER BY month\n           RANGE BETWEEN INTERVAL 2 MONTHS PRECEDING\n              AND INTERVAL 0 MONTHS FOLLOWING)\"\n\nbeta_sql <- sql(paste(\"regr_slope(ret_excess, mkt_excess)\", window))\nn_sql <- sql(paste(\"count(*)\", window))\n\nbeta_daily <- \n  crsp_daily |>\n  left_join(factors_ff_daily, by = \"date\") |>\n  select(permno, month, ret_excess, mkt_excess) |>\n  filter(!is.na(ret_excess)) |>\n  mutate(beta_daily = beta_sql, n = n_sql) |>\n  filter(n >= 50) |>\n  select(permno, month, beta_daily) |>\n  distinct() |>\n  compute()\n\ndbExecute(tidy_finance, \"DROP TABLE IF EXISTS beta_alt\")\n\nbeta_alt <- \n  beta_monthly |>\n  full_join(beta_daily, by = c(\"permno\", \"month\")) |>\n  arrange(permno, month) %>%\n  compute(name = \"beta_alt\", temporary = FALSE)\n\ndbDisconnect(tidy_finance, shutdown = TRUE)"
  },
  {
    "objectID": "betas.html#analysis-of-betas",
    "href": "betas.html#analysis-of-betas",
    "title": "4  Beta Estimation",
    "section": "4.2 Analysis of betas",
    "text": "4.2 Analysis of betas\nHaving created the table using the code below, I now include the analyses presented in the book, but using the database as the primary engine. Rendering this document takes less than six seconds.1\n\nlibrary(tidyverse)\nlibrary(DBI)\nlibrary(scales)\nlibrary(dbplyr)\n\n\ntidy_finance <- dbConnect(\n  duckdb::duckdb(),\n  \"data/tidy_finance.duckdb\",\n  read_only = TRUE)\n\n\nbeta <- tbl(tidy_finance, \"beta_alt\")\ncrsp_monthly <- tbl(tidy_finance, \"crsp_monthly\")\n\n\nexamples <- tribble(\n  ~permno, ~company,\n  14593, \"Apple\",\n  10107, \"Microsoft\",\n  93436, \"Tesla\",\n  17778, \"Berkshire Hathaway\"\n) |>\n  copy_inline(tidy_finance, df = _)\n\n\nbeta_examples <- \n  beta |>\n  inner_join(examples, by = \"permno\") |>\n  select(permno, company, month, beta_monthly) \n\nbeta_examples |>\n  filter(!is.na(beta_monthly)) |>\n  ggplot(aes(\n    x = month, \n    y = beta_monthly, \n    color = company,\n    linetype = company)) +\n  geom_line() +\n  labs(\n    x = NULL, y = NULL, color = NULL, linetype = NULL,\n    title = \"Monthly beta estimates for example stocks using 5 years of data\"\n  )\n\n\n\n\n\ncrsp_monthly |>\n  left_join(beta, join_by(permno, month)) |>\n  filter(!is.na(beta_monthly)) |>\n  group_by(industry, permno) |>\n  summarize(beta = mean(beta_monthly, na.rm = TRUE), \n            .groups = \"drop\") |>\n  ggplot(aes(x = reorder(industry, beta, FUN = median), y = beta)) +\n  geom_boxplot() +\n  coord_flip() +\n  labs(\n    x = NULL, y = NULL,\n    title = \"Firm-specific beta distributions by industry\"\n  )\n\n\n\n\n\nbeta |>\n  filter(!is.na(beta_monthly)) |>\n  group_by(month) |>\n  mutate(quantile = ntile(-beta_monthly, 10) * 10) |>\n  group_by(month, quantile) |>\n  summarize(x = min(beta_monthly, na.rm = TRUE), .groups = \"drop\") |>\n  filter(quantile != 100) |>\n  ggplot(aes(\n    x = month, \n    y = x, \n    color = as_factor(quantile),\n    linetype = as_factor(quantile)\n    )) +\n  geom_line() +\n  labs(\n    x = NULL, y = NULL, color = NULL, linetype = NULL,\n    title = \"Monthly deciles of estimated betas\",\n  )\n\n\n\n\n\nbeta |>\n  inner_join(examples, by = \"permno\") |>\n  pivot_longer(cols = c(beta_monthly, beta_daily)) |>\n  filter(!is.na(value)) |>\n  ggplot(aes(\n    x = month, \n    y = value, \n    color = name, \n    linetype = name\n    )) +\n  geom_line() +\n  facet_wrap(~ company, ncol = 1) +\n  labs(\n    x = NULL, y = NULL, color = NULL, linetype = NULL, \n    title = \"Comparison of beta estimates using monthly and daily data\"\n  )\n\n\n\n\n\nbeta_long <- \n  crsp_monthly |>\n  left_join(beta, by = c(\"permno\", \"month\")) |>\n  select(month, beta_monthly, beta_daily) |>\n  pivot_longer(cols = c(beta_monthly, beta_daily))\n\nbeta_long |>\n  group_by(month, name) |>\n  summarize(share = sum(as.double(!is.na(value)), na.rm = TRUE) / n(),\n            .groups = \"drop\") |>\n  ggplot(aes(\n    x = month, \n    y = share, \n    color = name,\n    linetype = name\n    )) +\n  geom_line() +\n  scale_y_continuous(labels = percent) +\n  labs(\n    x = NULL, y = NULL, color = NULL, linetype = NULL,\n    title = \"End-of-month share of securities with beta estimates\"\n  ) +\n  coord_cartesian(ylim = c(0, 1))\n\n\n\n\n\nbeta_long |>\n  select(name, value) |>\n  filter(!is.na(value)) |>\n  group_by(name) |>\n  summarize(\n    mean = mean(value),\n    sd = sd(value),\n    min = min(value),\n    q05 = quantile(value, 0.05),\n    q50 = quantile(value, 0.50),\n    q95 = quantile(value, 0.95),\n    max = max(value),\n    n = n()\n  ) |>\n  collect()\n\n# A tibble: 2 × 9\n  name          mean    sd   min    q05   q50   q95   max       n\n  <chr>        <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl>   <dbl>\n1 beta_monthly 1.10  0.715 -13.0  0.120 1.04   2.33  10.3 2214256\n2 beta_daily   0.749 0.927 -43.7 -0.448 0.685  2.23  56.6 3236855\n\n\n\ndbDisconnect(tidy_finance, shutdown = TRUE)"
  },
  {
    "objectID": "uni-sorts.html#data-preparation",
    "href": "uni-sorts.html#data-preparation",
    "title": "5  Univariate Portfolio Sorts",
    "section": "5.1 Data Preparation",
    "text": "5.1 Data Preparation\n\ntidy_finance <- dbConnect(\n  duckdb::duckdb(),\n  \"data/tidy_finance.duckdb\",\n  read_only = TRUE)\n\nThere are three tables used for this chapter. I defer the select() used with crsp_monthly in the book, as I find it more elegant to have a code chunk that just links to the database tables.\n\ncrsp_monthly <- tbl(tidy_finance, \"crsp_monthly\") \nfactors_ff_monthly <- tbl(tidy_finance, \"factors_ff_monthly\")\nbeta <- tbl(tidy_finance, \"beta_alt\")"
  },
  {
    "objectID": "uni-sorts.html#sorting-by-market-beta",
    "href": "uni-sorts.html#sorting-by-market-beta",
    "title": "5  Univariate Portfolio Sorts",
    "section": "5.2 Sorting by Market Beta",
    "text": "5.2 Sorting by Market Beta\n\nbeta_lag <- \n  beta |>\n  mutate(month = month + months(1)) |>\n  select(permno, month, beta_lag = beta_monthly) |>\n  filter(!is.na(beta_lag))\n\ndata_for_sorts <- \n  crsp_monthly |>\n  select(permno, month, ret_excess, mktcap_lag) |>\n  inner_join(beta_lag, by = c(\"permno\", \"month\"))\n\n\nbreakpoints <-\n  data_for_sorts |>\n  group_by(month) |>\n  summarize(breakpoint = median(beta_lag, na.rm = TRUE),\n            .groups = \"drop\")\n\nbeta_portfolios <- \n  data_for_sorts |>\n  inner_join(breakpoints, by = \"month\") |>\n  mutate(\n    portfolio = case_when(\n      beta_lag <= breakpoint ~ \"low\",\n      beta_lag > breakpoint ~ \"high\")) |>\n  filter(!is.na(ret_excess)) |>\n  group_by(month, portfolio) |>\n  summarize(ret = sum(ret_excess * mktcap_lag, na.rm = TRUE)/\n              sum(mktcap_lag, na.rm = TRUE), \n            .groups = \"drop\")"
  },
  {
    "objectID": "uni-sorts.html#performance-evaluation",
    "href": "uni-sorts.html#performance-evaluation",
    "title": "5  Univariate Portfolio Sorts",
    "section": "5.3 Performance Evaluation",
    "text": "5.3 Performance Evaluation\n\nbeta_longshort <- \n  beta_portfolios |>\n  pivot_wider(id_cols = month, names_from = portfolio, values_from = ret) |>\n  mutate(long_short = high - low) |>\n  collect()\n\n\nmodel_fit <- lm(long_short ~ 1, data = beta_longshort)\ncoeftest(model_fit, vcov = NeweyWest)\n\n\nt test of coefficients:\n\n              Estimate Std. Error t value Pr(>|t|)\n(Intercept) 0.00004227 0.00119452  0.0354   0.9718"
  },
  {
    "objectID": "uni-sorts.html#functional-programming-for-portfolio-sorts",
    "href": "uni-sorts.html#functional-programming-for-portfolio-sorts",
    "title": "5  Univariate Portfolio Sorts",
    "section": "5.4 Functional Programming for Portfolio Sorts",
    "text": "5.4 Functional Programming for Portfolio Sorts\nUsing the ntile() function, the assign_portfolio() function can be reduced to a one-liner. As such there is no reason to have a function, but I have retained this to illustrate the use of the curly-curly operator as shown in the book.\n\nassign_portfolio <- function(data, var, n_portfolios) {\n  mutate(data, portfolio = ntile({{ var }}, n_portfolios))\n}\n\nUsing a “data-frame-in-data-frame-out” function results in tidier code. Because the data are still in the database, we use our home-brewed alternative to weighted.mean() and defer the conversion of portfolio to a factor until we collect() the data below.\n\nbeta_portfolios <- \n  data_for_sorts |>\n  group_by(month) |>\n  assign_portfolio(var = beta_lag, 10) |>\n  group_by(portfolio, month) |>\n  summarize(ret = sum(ret_excess * mktcap_lag, na.rm = TRUE)/\n                    sum(mktcap_lag, na.rm = TRUE), \n            .groups = \"drop\")"
  },
  {
    "objectID": "uni-sorts.html#more-performance-evaluation",
    "href": "uni-sorts.html#more-performance-evaluation",
    "title": "5  Univariate Portfolio Sorts",
    "section": "5.5 More Performance Evaluation",
    "text": "5.5 More Performance Evaluation\nUnlike the book, I use the coef() function here for a small amount of convenience.\n\nbeta_portfolios_summary <- \n  beta_portfolios |>\n  left_join(factors_ff_monthly, by = \"month\") |>\n  group_by(portfolio) |>\n  collect() %>%\n  mutate(portfolio = as.factor(portfolio)) |>\n  summarize(\n    alpha = coef(lm(ret ~ 1 + mkt_excess))[1],\n    beta = coef(lm(ret ~ 1 + mkt_excess))[2],\n    ret = mean(ret)\n  )\n\n\nbeta_portfolios_summary |>\n  ggplot(aes(x = portfolio, y = alpha, fill = portfolio)) +\n  geom_bar(stat = \"identity\") +\n  labs(\n    title = \"CAPM alphas of beta-sorted portfolios\",\n    x = \"Portfolio\",\n    y = \"CAPM alpha\",\n    fill = \"Portfolio\"\n  ) +\n  scale_y_continuous(labels = percent) +\n  theme(legend.position = \"None\")\n\n\n\n\nFigure 5.1: Portfolios are sorted into deciles each month based on their estimated CAPM beta. The bar charts indicate the CAPM alpha of the resulting portfolio returns during the entire CRSP period."
  },
  {
    "objectID": "uni-sorts.html#the-security-market-line-and-beta-portfolios",
    "href": "uni-sorts.html#the-security-market-line-and-beta-portfolios",
    "title": "5  Univariate Portfolio Sorts",
    "section": "5.6 The Security Market Line and Beta Portfolios",
    "text": "5.6 The Security Market Line and Beta Portfolios\nAgain I use the coef() function and I also extract the calculation stored in mean_mkt_excess upfront.\n\nsml_capm <- coef(lm(ret ~ 1 + beta, data = beta_portfolios_summary))\n\nmean_mkt_excess <- \n  factors_ff_monthly |>\n  summarize(mean(mkt_excess, na.rm = TRUE)) |>\n  pull()\n\nbeta_portfolios_summary |>\n  filter(!is.na(ret), !is.na(beta)) |>\n  ggplot(aes(\n    x = beta, \n    y = ret, \n    color = portfolio\n  )) +\n  geom_point() +\n  geom_abline(\n    intercept = 0,\n    slope = mean_mkt_excess,\n    linetype = \"solid\"\n  ) +\n  geom_abline(\n    intercept = sml_capm[1],\n    slope = sml_capm[2],\n    linetype = \"dashed\"\n  ) +\n  scale_y_continuous(\n    labels = percent,\n    limit = c(0, mean_mkt_excess * 2)\n  ) +\n  scale_x_continuous(limits = c(0, 2)) +\n  labs(\n    x = \"Beta\", y = \"Excess return\", color = \"Portfolio\",\n    title = \"Average portfolio excess returns and average beta estimates\"\n  )\n\n\n\n\nFigure 5.2: Excess returns are computed as CAPM alphas of the beta-sorted portfolios. The horizontal axis indicates the CAPM beta of the resulting beta-sorted portfolio return time series. The dashed line indicates the slope coefficient of a linear regression of excess returns on portfolio betas.\n\n\n\n\nThe ungroup() shown in the book is not needed (because we used .groups = \"drop\") in the previous step.\n\nbeta_longshort <- \n  beta_portfolios |>\n  mutate(portfolio = case_when(\n    portfolio == max(as.numeric(portfolio)) ~ \"high\",\n    portfolio == min(as.numeric(portfolio)) ~ \"low\"\n  )) |>\n  filter(portfolio %in% c(\"low\", \"high\")) |>\n  pivot_wider(id_cols = month, names_from = portfolio, values_from = ret) |>\n  mutate(long_short = high - low) |>\n  left_join(factors_ff_monthly, by = \"month\") %>%\n  collect()\n\nWarning: Missing values are always removed in SQL aggregation functions.\nUse `na.rm = TRUE` to silence this warning\nThis warning is displayed once every 8 hours.\n\n\n\ncoeftest(lm(long_short ~ 1, data = beta_longshort),\n         vcov = NeweyWest)\n\n\nt test of coefficients:\n\n             Estimate Std. Error t value Pr(>|t|)\n(Intercept) 0.0021946  0.0024331   0.902   0.3674\n\n\n\ncoeftest(lm(long_short ~ 1 + mkt_excess, data = beta_longshort),\n         vcov = NeweyWest)\n\n\nt test of coefficients:\n\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.0046182  0.0021571 -2.1409  0.03263 *  \nmkt_excess   1.1699182  0.0675316 17.3240  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe have already applied collect() to beta_longshort, so we can use R functions here. If beta_longshort were a remote data frame, not that dbplyr does not translate prod() for us, but we could use the product() aggregate from DuckDB.\n\nbeta_longshort |>\n  group_by(year = year(month)) |>\n  summarize(\n    low = prod(1 + low),\n    high = prod(1 + high),\n    long_short = prod(1 + long_short)\n  ) |>\n  pivot_longer(cols = -year) |>\n  ggplot(aes(x = year, y = 1 - value, fill = name)) +\n  geom_col(position = \"dodge\") +\n  facet_wrap(~name, ncol = 1) +\n  theme(legend.position = \"none\") +\n  scale_y_continuous(labels = percent) +\n  labs(\n    title = \"Annual returns of beta portfolios\",\n    x = NULL, y = NULL\n  )\n\n\n\n\n\ndbDisconnect(tidy_finance, shutdown = TRUE)"
  },
  {
    "objectID": "size-sorts.html#data-preparation",
    "href": "size-sorts.html#data-preparation",
    "title": "6  Size Sorts and p-Hacking: Database version",
    "section": "6.1 Data Preparation",
    "text": "6.1 Data Preparation\n\ntidy_finance <- dbConnect(\n  duckdb::duckdb(),\n  \"data/tidy_finance.duckdb\",\n  read_only = TRUE)\n\ncrsp_monthly <- tbl(tidy_finance, \"crsp_monthly\")\nfactors_ff_monthly <- tbl(tidy_finance, \"factors_ff_monthly\")"
  },
  {
    "objectID": "size-sorts.html#size-distribution",
    "href": "size-sorts.html#size-distribution",
    "title": "6  Size Sorts and p-Hacking: Database version",
    "section": "6.2 Size Distribution",
    "text": "6.2 Size Distribution\n\nmonthly_quantiles <-\n  crsp_monthly |>\n  group_by(month) |>\n  summarize(\n    total_market_cap = sum(mktcap, na.rm = TRUE),\n    q_top01 = quantile_cont(mktcap, 0.99),\n    q_top05 = quantile_cont(mktcap, 0.95),\n    q_top10 = quantile_cont(mktcap, 0.90),\n    q_top25 = quantile_cont(mktcap, 0.75),\n    .groups = \"drop\")\n\ncrsp_monthly |>\n  inner_join(monthly_quantiles, by = \"month\") |>\n  mutate(\n    top01 = mktcap >= q_top01,\n    top05 = mktcap >= q_top05,\n    top10 = mktcap >= q_top10,\n    top25 = mktcap >= q_top25,\n    .groups = \"drop\") |>\n  group_by(month) |>\n  summarize(\n    `Largest 1% of stocks` = sum(mktcap[top01]/total_market_cap),\n    `Largest 5% of stocks` = sum(mktcap[top05]/total_market_cap),\n    `Largest 10% of stocks` = sum(mktcap[top10]/total_market_cap),\n    `Largest 25% of stocks` = sum(mktcap[top25]/total_market_cap),\n    .groups = \"drop\") |>\n  pivot_longer(cols = -month) |>\n  collect() |>\n  mutate(name = factor(name, levels = c(\n    \"Largest 1% of stocks\", \"Largest 5% of stocks\",\n    \"Largest 10% of stocks\", \"Largest 25% of stocks\"\n  ))) |>\n  ggplot(aes(\n    x = month, \n    y = value, \n    color = name,\n    linetype = name)) +\n  geom_line() +\n  scale_y_continuous(labels = percent, limits = c(0, 1)) +\n  labs(\n    x = NULL, y = NULL, color = NULL, linetype = NULL,\n    title = \"Percentage of total market capitalization in largest stocks\")\n\n\n\n\n\ncrsp_monthly |>\n  group_by(month, exchange) |>\n  summarize(mktcap = sum(mktcap),\n            .groups = \"drop_last\") |>\n  mutate(share = mktcap / sum(mktcap)) |>\n  ggplot(aes(\n    x = month, \n    y = share, \n    fill = exchange, \n    color = exchange)) +\n  geom_area(\n    position = \"stack\",\n    stat = \"identity\",\n    alpha = 0.5\n  ) +\n  geom_line(position = \"stack\") +\n  scale_y_continuous(labels = percent) +\n  labs(\n    x = NULL, y = NULL, fill = NULL, color = NULL,\n    title = \"Share of total market capitalization per listing exchange\"\n  )\n\n\n\n\n\ncreate_summary <- function(data, column_name) {\n  data |>\n    mutate(value = {{ column_name }}) |>\n    summarize(\n      mean = mean(value),\n      sd = sd(value),\n      min = min(value),\n      q05 = quantile_cont(value, 0.05),\n      q50 = quantile_cont(value, 0.50),\n      q95 = quantile_cont(value, 0.95),\n      max = max(value),\n      n = n()\n    )\n}\n\ncrsp_monthly |>\n  filter(month == max(month)) |>\n  group_by(exchange) |>\n  create_summary(mktcap) |>\n  arrange(exchange) |>\n  union_all(crsp_monthly |>\n            filter(month == max(month)) |>\n            create_summary(mktcap) |>\n            mutate(exchange = \"Overall\")) |>\n  collect()\n\n# A tibble: 5 × 9\n  exchange   mean     sd      min     q05     q50    q95      max     n\n  <chr>     <dbl>  <dbl>    <dbl>   <dbl>   <dbl>  <dbl>    <dbl> <dbl>\n1 AMEX       415.  2181.     7.57    12.6    75.8  1218.   25719.   145\n2 NASDAQ    8651. 90038.     7.01    29.3   429.  18781. 2902368.  2779\n3 NYSE     17858. 48619.    23.9    195.   3434.  80748.  472941.  1395\n4 Other    13906.    NA  13906.   13906.  13906.  13906.   13906.     1\n5 Overall  11349. 77458.     7.01    34.3   796.  40647. 2902368.  4320\n\n\n\nassign_portfolio <- function(data, n_portfolios, exchanges) {\n  data |>\n    filter(exchange %in% exchanges) |>\n    mutate(portfolio = ntile(mktcap_lag, n_portfolios))\n}\n\n\ncompute_portfolio_returns <- function(data = crsp_monthly,\n                                      n_portfolios = 10,\n                                      exchanges = c(\"NYSE\", \"NASDAQ\", \"AMEX\"),\n                                      value_weighted = TRUE) {\n  \n  exchanges_str <- str_flatten(exchanges, \", \")\n  \n  data |>\n    group_by(month) |>\n    assign_portfolio(n_portfolios, exchanges) |>\n    group_by(month, portfolio) |>\n    filter(!is.na(ret), !is.na(mktcap_lag)) |>\n    mutate(weight = if_else(value_weighted, mktcap_lag, 1)) |>\n    summarize(\n       ret = sum(ret_excess * weight, na.rm = TRUE)/\n                    sum(weight, na.rm = TRUE),\n      .groups = \"drop\") |>\n    mutate(portfolio = case_when(portfolio == min(portfolio) ~ \"min\",\n                                 portfolio == max(portfolio) ~ \"max\",\n                                 TRUE ~ \"other\")) |>\n    filter(portfolio %in% c(\"min\", \"max\")) |>\n    pivot_wider(names_from = portfolio, values_from = ret) |>\n    mutate(size_premium = min - max) |>\n    summarize(Premium = mean(size_premium)) |>\n    mutate(Exchanges = exchanges_str) |>\n    select(Exchanges, Premium) |>\n    collect()\n}\n\n\nret_all <- \n  crsp_monthly |>\n  compute_portfolio_returns(n_portfolios = 2,\n                            exchanges = c(\"NYSE\", \"NASDAQ\", \"AMEX\"),\n                            value_weighted = TRUE) \n\nret_nyse <- \n  crsp_monthly |>\n  compute_portfolio_returns(n_portfolios = 2,\n                            exchanges = \"NYSE\",\n                            value_weighted = TRUE)\n\nbind_rows(ret_all, ret_nyse)\n\n# A tibble: 2 × 2\n  Exchanges           Premium\n  <chr>                 <dbl>\n1 NYSE, NASDAQ, AMEX 0.000977\n2 NYSE               0.00328 \n\n\n\np_hacking_setup <- \n  expand_grid(\n    n_portfolios = c(2, 5, 10),\n    exchanges = list(\"NYSE\", c(\"NYSE\", \"NASDAQ\", \"AMEX\")),\n    value_weighted = c(TRUE, FALSE),\n    data = parse_exprs(\n     'crsp_monthly; \n      crsp_monthly |> filter(industry != \"Finance\");\n      crsp_monthly |> filter(month < \"1990-06-01\");\n      crsp_monthly |> filter(month >=\"1990-06-01\")'))\n\n\np_hacking_setup <- \n  p_hacking_setup |>\n  mutate(size_premium = \n           pmap(.l = list(data, n_portfolios, exchanges, value_weighted),\n                .f = ~ compute_portfolio_returns(\n                  data = eval_tidy(..1),\n                  n_portfolios = ..2,\n                  exchanges = ..3,\n                  value_weighted = ..4)))\n\n\np_hacking_results <- \n  p_hacking_setup |>\n  mutate(data = map_chr(data, deparse)) |>\n  unnest(size_premium) |>\n  arrange(desc(Premium))\n\np_hacking_results\n\n# A tibble: 48 × 6\n   n_portfolios exchanges value_weighted data                  Exchanges Premium\n          <dbl> <list>    <lgl>          <chr>                 <chr>       <dbl>\n 1           10 <chr [3]> FALSE          \"filter(crsp_monthly… NYSE, NA… 0.0186 \n 2           10 <chr [3]> FALSE          \"filter(crsp_monthly… NYSE, NA… 0.0182 \n 3           10 <chr [3]> FALSE          \"crsp_monthly\"        NYSE, NA… 0.0163 \n 4           10 <chr [3]> FALSE          \"filter(crsp_monthly… NYSE, NA… 0.0140 \n 5           10 <chr [3]> TRUE           \"filter(crsp_monthly… NYSE, NA… 0.0115 \n 6           10 <chr [3]> TRUE           \"filter(crsp_monthly… NYSE, NA… 0.0109 \n 7           10 <chr [3]> TRUE           \"crsp_monthly\"        NYSE, NA… 0.0103 \n 8           10 <chr [3]> TRUE           \"filter(crsp_monthly… NYSE, NA… 0.00968\n 9            5 <chr [3]> FALSE          \"filter(crsp_monthly… NYSE, NA… 0.00926\n10            5 <chr [3]> FALSE          \"filter(crsp_monthly… NYSE, NA… 0.00895\n# ℹ 38 more rows\n\n\n\nsmb_mean <- \n  factors_ff_monthly |> \n  summarize(mean(smb, na.rm = TRUE)) |>\n  pull()\n\np_hacking_results |>\n  filter(!is.na(Premium)) |>\n  ggplot(aes(x = Premium)) +\n  geom_histogram(bins = nrow(p_hacking_results)) +\n  labs(\n    x = NULL, y = NULL,\n    title = \"Distribution of size premiums for different sorting choices\"\n  ) +\n  geom_vline(aes(xintercept = smb_mean),\n    linetype = \"dashed\"\n  ) +\n  scale_x_continuous(labels = percent)\n\n\n\n\n\ndbDisconnect(tidy_finance, shutdown = TRUE)"
  },
  {
    "objectID": "size-sorts-orig.html#data-preparation",
    "href": "size-sorts-orig.html#data-preparation",
    "title": "7  Size Sorts and p-Hacking: Original code",
    "section": "7.1 Data Preparation",
    "text": "7.1 Data Preparation\n\ntidy_finance <- dbConnect(\n  duckdb::duckdb(),\n  \"data/tidy_finance.duckdb\",\n  read_only = TRUE)\n\ncrsp_monthly <- tbl(tidy_finance, \"crsp_monthly\") |> collect()\nfactors_ff_monthly <- tbl(tidy_finance, \"factors_ff_monthly\") |> collect()"
  },
  {
    "objectID": "size-sorts-orig.html#size-distribution",
    "href": "size-sorts-orig.html#size-distribution",
    "title": "7  Size Sorts and p-Hacking: Original code",
    "section": "7.2 Size Distribution",
    "text": "7.2 Size Distribution\n\ncrsp_monthly |>\n  group_by(month) |>\n  mutate(\n    top01 = if_else(mktcap >= quantile(mktcap, 0.99), 1, 0),\n    top05 = if_else(mktcap >= quantile(mktcap, 0.95), 1, 0),\n    top10 = if_else(mktcap >= quantile(mktcap, 0.90), 1, 0),\n    top25 = if_else(mktcap >= quantile(mktcap, 0.75), 1, 0)\n  ) |>\n  summarize(\n    total_market_cap =  sum(mktcap),\n    `Largest 1% of stocks` = sum(mktcap[top01 == 1]) / total_market_cap,\n    `Largest 5% of stocks` = sum(mktcap[top05 == 1]) / total_market_cap,\n    `Largest 10% of stocks` = sum(mktcap[top10 == 1]) / total_market_cap,\n    `Largest 25% of stocks` = sum(mktcap[top25 == 1]) / total_market_cap,\n    .groups = \"drop\"\n  ) |>\n  select(-total_market_cap) |> \n  pivot_longer(cols = -month) |>\n  mutate(name = factor(name, levels = c(\n    \"Largest 1% of stocks\", \"Largest 5% of stocks\",\n    \"Largest 10% of stocks\", \"Largest 25% of stocks\"\n  ))) |>\n  ggplot(aes(\n    x = month, \n    y = value, \n    color = name,\n    linetype = name)) +\n  geom_line() +\n  scale_y_continuous(labels = percent, limits = c(0, 1)) +\n  labs(\n    x = NULL, y = NULL, color = NULL, linetype = NULL,\n    title = \"Percentage of total market capitalization in largest stocks\"\n  )\n\n\n\n\n\ncrsp_monthly |>\n  group_by(month, exchange) |>\n  summarize(mktcap = sum(mktcap),\n            .groups = \"drop_last\") |>\n  mutate(share = mktcap / sum(mktcap)) |>\n  ggplot(aes(\n    x = month, \n    y = share, \n    fill = exchange, \n    color = exchange)) +\n  geom_area(\n    position = \"stack\",\n    stat = \"identity\",\n    alpha = 0.5\n  ) +\n  geom_line(position = \"stack\") +\n  scale_y_continuous(labels = percent) +\n  labs(\n    x = NULL, y = NULL, fill = NULL, color = NULL,\n    title = \"Share of total market capitalization per listing exchange\"\n  )\n\n\n\n\n\ncreate_summary <- function(data, column_name) {\n  data |>\n    select(value = {{ column_name }}) |>\n    summarize(\n      mean = mean(value),\n      sd = sd(value),\n      min = min(value),\n      q05 = quantile(value, 0.05),\n      q50 = quantile(value, 0.50),\n      q95 = quantile(value, 0.95),\n      max = max(value),\n      n = n()\n    )\n}\n\ncrsp_monthly |>\n  filter(month == max(month)) |>\n  group_by(exchange) |>\n  create_summary(mktcap) |>\n  add_row(crsp_monthly |>\n            filter(month == max(month)) |>\n            create_summary(mktcap) |>\n            mutate(exchange = \"Overall\"))\n\nAdding missing grouping variables: `exchange`\n\n\n# A tibble: 5 × 9\n  exchange   mean     sd      min     q05     q50    q95      max     n\n  <chr>     <dbl>  <dbl>    <dbl>   <dbl>   <dbl>  <dbl>    <dbl> <int>\n1 AMEX       415.  2181.     7.57    12.6    75.8  1218.   25719.   145\n2 NASDAQ    8651. 90038.     7.01    29.3   429.  18781. 2902368.  2779\n3 NYSE     17858. 48619.    23.9    195.   3434.  80748.  472941.  1395\n4 Other    13906.    NA  13906.   13906.  13906.  13906.   13906.     1\n5 Overall  11349. 77458.     7.01    34.3   796.  40647. 2902368.  4320\n\n\n\nassign_portfolio <- function(n_portfolios,\n                             exchanges,\n                             data) {\n  breakpoints <- data |>\n    filter(exchange %in% exchanges) |>\n    reframe(breakpoint = quantile(\n      mktcap_lag,\n      probs = seq(0, 1, length.out = n_portfolios + 1),\n      na.rm = TRUE\n    )) |>\n    pull(breakpoint) |>\n    as.numeric()\n\n  assigned_portfolios <- data |>\n    mutate(portfolio = findInterval(mktcap_lag,\n      breakpoints,\n      all.inside = TRUE\n    )) |>\n    pull(portfolio)\n  return(assigned_portfolios)\n}\n\n\ncompute_portfolio_returns <- function(n_portfolios = 10,\n                                      exchanges = c(\"NYSE\", \"NASDAQ\", \"AMEX\"),\n                                      value_weighted = TRUE,\n                                      data = crsp_monthly) {\n  data |>\n    group_by(month) |>\n    mutate(portfolio = assign_portfolio(\n      n_portfolios = n_portfolios,\n      exchanges = exchanges,\n      data = pick(everything())\n    )) |>\n    group_by(month, portfolio) |>\n    summarize(\n      ret = if_else(value_weighted,\n        weighted.mean(ret_excess, mktcap_lag),\n        mean(ret_excess)\n      ),\n      .groups = \"drop_last\"\n    ) |>\n    summarize(size_premium = ret[portfolio == min(portfolio)] -\n      ret[portfolio == max(portfolio)]) |>\n    summarize(size_premium = mean(size_premium))\n}\n\n\nret_all <- compute_portfolio_returns(\n  n_portfolios = 2,\n  exchanges = c(\"NYSE\", \"NASDAQ\", \"AMEX\"),\n  value_weighted = TRUE,\n  data = crsp_monthly\n)\n\nret_nyse <- compute_portfolio_returns(\n  n_portfolios = 2,\n  exchanges = \"NYSE\",\n  value_weighted = TRUE,\n  data = crsp_monthly\n)\n\ntibble(\n  Exchanges = c(\"NYSE, NASDAQ & AMEX\", \"NYSE\"),\n  Premium = as.numeric(c(ret_all, ret_nyse)) * 100\n)\n\n# A tibble: 2 × 2\n  Exchanges           Premium\n  <chr>                 <dbl>\n1 NYSE, NASDAQ & AMEX -0.0320\n2 NYSE                 0.108 \n\n\n\np_hacking_setup <- \n  expand_grid(\n    n_portfolios = c(2, 5, 10),\n    exchanges = list(\"NYSE\", c(\"NYSE\", \"NASDAQ\", \"AMEX\")),\n    value_weighted = c(TRUE, FALSE),\n    data = parse_exprs(\n     'crsp_monthly; \n      crsp_monthly |> filter(industry != \"Finance\");\n      crsp_monthly |> filter(month < \"1990-06-01\");\n      crsp_monthly |> filter(month >=\"1990-06-01\")'))\n\n\nplan(multisession, workers = availableCores())\n\np_hacking_setup <- p_hacking_setup |>\n  mutate(size_premium = future_pmap(\n    .l = list(\n      n_portfolios,\n      exchanges,\n      value_weighted,\n      data\n    ),\n    .f = ~ compute_portfolio_returns(\n      n_portfolios = ..1,\n      exchanges = ..2,\n      value_weighted = ..3,\n      data = eval_tidy(..4)\n    )\n  ))\n\n\np_hacking_results <- p_hacking_setup |>\n  mutate(data = map_chr(data, deparse)) |>\n  unnest(size_premium) |>\n  arrange(desc(size_premium))\n\np_hacking_results\n\n# A tibble: 48 × 5\n   n_portfolios exchanges value_weighted data                       size_premium\n          <dbl> <list>    <lgl>          <chr>                             <dbl>\n 1           10 <chr [3]> FALSE          \"filter(crsp_monthly, mon…      0.0164 \n 2           10 <chr [3]> FALSE          \"filter(crsp_monthly, ind…      0.0134 \n 3           10 <chr [3]> FALSE          \"crsp_monthly\"                  0.0116 \n 4           10 <chr [3]> TRUE           \"filter(crsp_monthly, mon…      0.00782\n 5            5 <chr [3]> FALSE          \"filter(crsp_monthly, mon…      0.00740\n 6           10 <chr [3]> TRUE           \"filter(crsp_monthly, ind…      0.00729\n 7           10 <chr [3]> FALSE          \"filter(crsp_monthly, mon…      0.00667\n 8           10 <chr [3]> TRUE           \"crsp_monthly\"                  0.00612\n 9            5 <chr [3]> FALSE          \"filter(crsp_monthly, ind…      0.00568\n10            5 <chr [3]> FALSE          \"crsp_monthly\"                  0.00472\n# ℹ 38 more rows\n\n\n\np_hacking_results |>\n  ggplot(aes(x = size_premium)) +\n  geom_histogram(bins = nrow(p_hacking_results)) +\n  labs(\n    x = NULL, y = NULL,\n    title = \"Distribution of size premiums for different sorting choices\"\n  ) +\n  geom_vline(aes(xintercept = mean(factors_ff_monthly$smb)),\n    linetype = \"dashed\"\n  ) +\n  scale_x_continuous(labels = percent)\n\n\n\n\n\ndbDisconnect(tidy_finance, shutdown = TRUE)"
  }
]